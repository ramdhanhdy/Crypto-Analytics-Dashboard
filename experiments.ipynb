{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHLCV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from binance.client import Client\n",
    "\n",
    "# Load API keys from config.json\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "API_KEY = config[\"binance\"][\"api_key\"]\n",
    "API_SECRET = config[\"binance\"][\"api_secret\"]\n",
    "\n",
    "# Initialize Binance client\n",
    "client = Client(API_KEY, API_SECRET)\n",
    "\n",
    "# Fetch OHLCV data\n",
    "def fetch_ohlcv(symbol=\"BTCUSDT\", interval=\"5m\", limit=500):\n",
    "    data = client.get_klines(symbol=symbol, interval=interval, limit=limit)\n",
    "    df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', \n",
    "                                     'close_time', 'quote_volume', 'trades', \n",
    "                                     'taker_base_vol', 'taker_quote_vol', 'ignore'])\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].astype(float)\n",
    "    \n",
    "    return df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "ohlcv_df = fetch_ohlcv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orderbook Data (Liquidity & Spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_order_book(symbol=\"BTCUSDT\", depth=10):\n",
    "    order_book = client.get_order_book(symbol=symbol, limit=depth)\n",
    "    \n",
    "    bids = pd.DataFrame(order_book['bids'], columns=['bid_price', 'bid_size']).astype(float)\n",
    "    asks = pd.DataFrame(order_book['asks'], columns=['ask_price', 'ask_size']).astype(float)\n",
    "    \n",
    "    # Compute bid-ask spread\n",
    "    spread = asks['ask_price'].iloc[0] - bids['bid_price'].iloc[0]\n",
    "\n",
    "    return {'bids': bids, 'asks': asks, 'spread': spread}\n",
    "\n",
    "order_book_data = fetch_order_book()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.__sizeof__>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_book_data.__sizeof__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funding Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              fundingTime   fundingRate\n",
      "0 2024-12-29 08:00:00.000  1.000000e-04\n",
      "1 2024-12-29 16:00:00.000  1.000000e-04\n",
      "2 2024-12-30 00:00:00.000 -4.200000e-07\n",
      "3 2024-12-30 08:00:00.001  8.791000e-05\n",
      "4 2024-12-30 16:00:00.000  1.000000e-04\n"
     ]
    }
   ],
   "source": [
    "def fetch_historical_funding_rates(symbol=\"BTCUSDT\", limit=100):\n",
    "    url = f\"https://fapi.binance.com/fapi/v1/fundingRate?symbol={symbol}&limit={limit}\"\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    if isinstance(response, list):\n",
    "        df = pd.DataFrame(response)\n",
    "        df['fundingTime'] = pd.to_datetime(df['fundingTime'], unit='ms')\n",
    "        df['fundingRate'] = df['fundingRate'].astype(float)\n",
    "        return df[['fundingTime', 'fundingRate']]\n",
    "    else:\n",
    "        print(f\"Error fetching funding rates: {response}\")\n",
    "        return None\n",
    "\n",
    "funding_df = fetch_historical_funding_rates()\n",
    "print(funding_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp       open       high        low      close     volume  \\\n",
      "0 2025-01-29 20:35:00  104488.64  104512.00  104066.66  104127.91  209.06861   \n",
      "1 2025-01-29 20:40:00  104127.92  104296.00  103945.75  104273.26  174.54813   \n",
      "2 2025-01-29 20:45:00  104273.26  104384.00  104090.91  104278.96  128.04634   \n",
      "3 2025-01-29 20:50:00  104278.95  104450.52  104016.84  104303.87  122.39772   \n",
      "4 2025-01-29 20:55:00  104303.86  104484.70  104187.53  104193.93  156.66794   \n",
      "\n",
      "          fundingTime  fundingRate  \n",
      "0 2025-01-29 16:00:00     0.000099  \n",
      "1 2025-01-29 16:00:00     0.000099  \n",
      "2 2025-01-29 16:00:00     0.000099  \n",
      "3 2025-01-29 16:00:00     0.000099  \n",
      "4 2025-01-29 16:00:00     0.000099  \n"
     ]
    }
   ],
   "source": [
    "# Merge funding rate with OHLCV data\n",
    "ohlcv_df = fetch_ohlcv()\n",
    "\n",
    "# Convert timestamps for proper merging\n",
    "ohlcv_df['timestamp'] = pd.to_datetime(ohlcv_df['timestamp'])\n",
    "funding_df['fundingTime'] = pd.to_datetime(funding_df['fundingTime'])\n",
    "\n",
    "# Merge based on nearest timestamps\n",
    "ohlcv_df = pd.merge_asof(ohlcv_df.sort_values('timestamp'), \n",
    "                          funding_df.sort_values('fundingTime'),\n",
    "                          left_on='timestamp', right_on='fundingTime')\n",
    "\n",
    "print(ohlcv_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fundingTime</th>\n",
       "      <th>fundingRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-29 08:00:00.000</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-29 16:00:00.000</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-30 00:00:00.000</td>\n",
       "      <td>-4.200000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-30 08:00:00.001</td>\n",
       "      <td>8.791000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-30 16:00:00.000</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2025-01-30 00:00:00.000</td>\n",
       "      <td>7.590000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2025-01-30 08:00:00.000</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2025-01-30 16:00:00.000</td>\n",
       "      <td>9.499000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2025-01-31 00:00:00.000</td>\n",
       "      <td>1.225000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2025-01-31 08:00:00.000</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               fundingTime   fundingRate\n",
       "0  2024-12-29 08:00:00.000  1.000000e-04\n",
       "1  2024-12-29 16:00:00.000  1.000000e-04\n",
       "2  2024-12-30 00:00:00.000 -4.200000e-07\n",
       "3  2024-12-30 08:00:00.001  8.791000e-05\n",
       "4  2024-12-30 16:00:00.000  1.000000e-04\n",
       "..                     ...           ...\n",
       "95 2025-01-30 00:00:00.000  7.590000e-05\n",
       "96 2025-01-30 08:00:00.000  1.000000e-04\n",
       "97 2025-01-30 16:00:00.000  9.499000e-05\n",
       "98 2025-01-31 00:00:00.000  1.225000e-05\n",
       "99 2025-01-31 08:00:00.000  1.000000e-04\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Stability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fractional_diff(series, d=0.4, threshold=1e-5):\n",
    "    weights = [1]\n",
    "    for k in range(1, len(series)):\n",
    "        w = -weights[-1] * ((d - k + 1) / k)\n",
    "        if abs(w) < threshold:\n",
    "            break\n",
    "        weights.append(w)\n",
    "    weights = np.array(weights[::-1]).reshape(-1, 1)\n",
    "    \n",
    "    return series.rolling(len(weights)).apply(lambda x: np.dot(weights.T, x), raw=True)\n",
    "\n",
    "ohlcv_df['frac_diff_returns'] = fractional_diff(ohlcv_df['close'].pct_change())\n",
    "\n",
    "# Rolling correlation to check stability\n",
    "ohlcv_df['frac_diff_corr'] = ohlcv_df['frac_diff_returns'].rolling(100).corr(ohlcv_df['close'].pct_change())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OrderFlow Imbalance (OFI) Stability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_flow_imbalance(bids, asks):\n",
    "    buy_pressure = asks['ask_size'].sum()\n",
    "    sell_pressure = bids['bid_size'].sum()\n",
    "    \n",
    "    return (buy_pressure - sell_pressure) / (buy_pressure + sell_pressure)\n",
    "\n",
    "ohlcv_df['ofi'] = order_flow_imbalance(order_book_data['bids'], order_book_data['asks'])\n",
    "\n",
    "# Check variance over time\n",
    "ohlcv_df['ofi_var'] = ohlcv_df['ofi'].rolling(100).var()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Power Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, weâ€™ll use XGBoost to see which features are important for position sizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Dataset ===\n",
      "OHLCV rows: 1000\n",
      "Final rows (post-feature): 1000\n",
      "\n",
      "=== Training XGBoost for Next-Candle Return Prediction ===\n",
      "Before dropna, rows: 1000\n",
      "After dropna, rows: 949\n",
      "Test MSE: 0.00000158\n",
      "Test R^2: -0.114555\n",
      "Feature Importance: {'frac_diff_returns': 208.0, 'entropy': 119.0, 'skew_30': 199.0, 'kurt_30': 160.0, 'hurst_50': 155.0, 'perm_entropy_10': 50.0, 'wavelet_energy': 126.0, 'lyapunov_50': 115.0, 'tsallis_30': 103.0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from binance.client import Client\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pywt  # For wavelet transforms\n",
    "\n",
    "##############################################\n",
    "# 1. Load API Keys & Initialize Client\n",
    "##############################################\n",
    "\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "API_KEY = config[\"binance\"][\"api_key\"]\n",
    "API_SECRET = config[\"binance\"][\"api_secret\"]\n",
    "client = Client(API_KEY, API_SECRET)\n",
    "\n",
    "##############################################\n",
    "# 2. Advanced/Exotic Feature Functions\n",
    "##############################################\n",
    "\n",
    "def fractional_diff(series, d=0.4, max_lags=50):\n",
    "    \"\"\"Fractionally differentiate 'series' using up to `max_lags` terms.\"\"\"\n",
    "    w = [1.0]\n",
    "    for k in range(1, max_lags):\n",
    "        w_ = -w[-1] * ((d - (k - 1)) / k)\n",
    "        w.append(w_)\n",
    "    w = np.array(w[::-1]).reshape(-1, 1)\n",
    "\n",
    "    def fracdiff_apply(x):\n",
    "        return np.dot(w.T, x)[0]\n",
    "\n",
    "    return series.rolling(window=max_lags).apply(fracdiff_apply, raw=True)\n",
    "\n",
    "def rolling_shannon_entropy(series, window=5, bins=5):\n",
    "    \"\"\"Rolling Shannon Entropy of 'series' distribution in each window.\"\"\"\n",
    "    def shannon_window(x):\n",
    "        hist, _ = np.histogram(x, bins=bins, density=True)\n",
    "        hist = hist[hist > 0]\n",
    "        return -np.sum(hist * np.log2(hist))\n",
    "    \n",
    "    return series.rolling(window).apply(shannon_window, raw=False)\n",
    "\n",
    "def rolling_skewness(series, window=30):\n",
    "    \"\"\"Rolling skewness (3rd moment).\"\"\"\n",
    "    return series.rolling(window).apply(lambda x: skew(x), raw=False)\n",
    "\n",
    "def rolling_kurtosis(series, window=30):\n",
    "    \"\"\"Rolling kurtosis (4th moment).\"\"\"\n",
    "    return series.rolling(window).apply(lambda x: kurtosis(x), raw=False)\n",
    "\n",
    "def rolling_hurst_exponent(series, window=50):\n",
    "    \"\"\"\n",
    "    Rolling Hurst exponent with naive R/S approach.\n",
    "    H > 0.5 => persistent/trending, < 0.5 => mean-reverting, ~0.5 => random.\n",
    "    \"\"\"\n",
    "    def hurst_rs(x):\n",
    "        x = pd.Series(x)\n",
    "        mean_x = x.mean()\n",
    "        adj = x - mean_x\n",
    "        cum = adj.cumsum()\n",
    "        R = cum.max() - cum.min()\n",
    "        S = x.std() if x.std() != 0 else 1e-9\n",
    "        N = len(x)\n",
    "        return np.log(R/S)/np.log(N) if R > 0 else 0.0\n",
    "\n",
    "    return series.rolling(window).apply(hurst_rs, raw=False)\n",
    "\n",
    "##############################################\n",
    "# 2.1. Even More Advanced (Chaos / Physics)\n",
    "##############################################\n",
    "\n",
    "def rolling_perm_entropy(series, window=10, order=3):\n",
    "    \"\"\"\n",
    "    Rolling **Permutation Entropy**. \n",
    "    - 'order' is the embedding dimension used for ordinal patterns.\n",
    "    - If series is random, PE ~ ln(order!), \n",
    "      if series is perfectly predictable, PE is smaller.\n",
    "    - We'll compute it for each rolling window of length 'window'.\n",
    "    \"\"\"\n",
    "    def perm_entropy_window(x):\n",
    "        # x is the local window\n",
    "        # generate ordinal patterns of length = 'order'\n",
    "        # for simplicity, we skip overlapping patterns beyond what's needed\n",
    "        if len(x) < order:\n",
    "            return np.nan\n",
    "        \n",
    "        # Create permutations\n",
    "        # We'll just do the simplest: consecutive ordinal patterns\n",
    "        # e.g. x[i:i+order], and see how often each pattern occurs\n",
    "        patterns_count = {}\n",
    "        for i in range(len(x) - order + 1):\n",
    "            # ordinal pattern\n",
    "            sub = x[i:i+order]\n",
    "            # get indices that would sort 'sub'\n",
    "            pattern = tuple(np.argsort(sub))\n",
    "            patterns_count[pattern] = patterns_count.get(pattern, 0) + 1\n",
    "        \n",
    "        total = sum(patterns_count.values())\n",
    "        # Shannon entropy of the distribution of patterns\n",
    "        pe = 0.0\n",
    "        for c in patterns_count.values():\n",
    "            p = c / total\n",
    "            pe -= p * np.log2(p)\n",
    "        return pe\n",
    "\n",
    "    return series.rolling(window).apply(perm_entropy_window, raw=False)\n",
    "\n",
    "def rolling_wavelet_energy(series, wavelet='haar', level=2, window=50):\n",
    "    \"\"\"\n",
    "    Rolling wavelet energy. \n",
    "    1) For each rolling window, do a discrete wavelet transform up to 'level'\n",
    "    2) Sum of squares of detail coeffs => 'wavelet energy'\n",
    "    More advanced versions might sum across multiple decomposition levels or do continuous wavelet transforms.\n",
    "    \"\"\"\n",
    "    def wavelet_window(x):\n",
    "        # Basic DWT\n",
    "        # pywt.wavedec returns a list [cA_{level}, cD_{level}, ..., cD_{1}]\n",
    "        coeffs = pywt.wavedec(x, wavelet, level=level)\n",
    "        # We can measure energy as sum of squares of detail coefficients\n",
    "        # e.g. cD_1, cD_2, etc.\n",
    "        energy = 0.0\n",
    "        # skip the approximation at index 0, use detail coeffs from index 1\n",
    "        for c in coeffs[1:]:\n",
    "            energy += np.sum(np.array(c)**2)\n",
    "        return energy\n",
    "    \n",
    "    return series.rolling(window).apply(wavelet_window, raw=True)\n",
    "\n",
    "def rolling_lyapunov_exponent(series, window=50, tau=1, dim=2):\n",
    "    \"\"\"\n",
    "    Approximate Largest Lyapunov Exponent (LLE) in a rolling window \n",
    "    using a simple Rosenstein method.\n",
    "    This is a *very rough* approach.\n",
    "    - 'tau' is time delay\n",
    "    - 'dim' is embedding dimension\n",
    "    Realistically, you'd want a more robust library or method. \n",
    "    \"\"\"\n",
    "    def lyapunov_window(x):\n",
    "        x = np.array(x)\n",
    "        # embed the time series in dimension=dim, time delay=tau\n",
    "        # We'll do minimal checks\n",
    "        N = len(x) - (dim-1)*tau\n",
    "        if N < 2:\n",
    "            return np.nan\n",
    "\n",
    "        # Build embedded vectors\n",
    "        embedded = []\n",
    "        for i in range(N):\n",
    "            # each vector is [x[i], x[i+tau], ..., x[i+(dim-1)*tau]]\n",
    "            v = x[i : i + dim*tau : tau]\n",
    "            embedded.append(v)\n",
    "        embedded = np.array(embedded)\n",
    "\n",
    "        # For each point, find nearest neighbor\n",
    "        # This is O(N^2) but okay for small windows\n",
    "        dists = []\n",
    "        for i in range(N):\n",
    "            dmin = 1e20\n",
    "            for j in range(N):\n",
    "                if j == i: \n",
    "                    continue\n",
    "                dist_ij = np.linalg.norm(embedded[i] - embedded[j])\n",
    "                if dist_ij < dmin:\n",
    "                    dmin = dist_ij\n",
    "            dists.append(dmin)\n",
    "        \n",
    "        # For a real method, we'd track how distances evolve over time steps,\n",
    "        # but let's do a naive measure: average log(dmin)\n",
    "        avg_log_dist = np.mean(np.log(np.array(dists) + 1e-9))\n",
    "        # We'll treat that as a proxy for chaos => higher => more chaotic\n",
    "        return avg_log_dist\n",
    "\n",
    "    return series.rolling(window).apply(lyapunov_window, raw=False)\n",
    "\n",
    "def rolling_tsallis_entropy(series, window=30, q=1.5):\n",
    "    \"\"\"\n",
    "    Tsallis entropy: a generalized entropy measure from non-extensive thermodynamics.\n",
    "    For q=1, it becomes Shannon. For q!=1, there's a 'q' parameter controlling concavity.\n",
    "    We'll compute it in a naive histogram approach for each rolling window.\n",
    "    \"\"\"\n",
    "    def tsallis_window(x):\n",
    "        hist, _ = np.histogram(x, bins=20, density=True)\n",
    "        hist = hist[hist>0]\n",
    "        # Tsallis S_q = (1 - sum(p_i^q)) / (q - 1)\n",
    "        p_q = hist**q\n",
    "        return (1.0 - np.sum(p_q)) / (q - 1.0)\n",
    "    \n",
    "    return series.rolling(window).apply(tsallis_window, raw=False)\n",
    "\n",
    "##############################################\n",
    "# 3. Fetching OHLCV with get_historical_klines\n",
    "##############################################\n",
    "\n",
    "def fetch_ohlcv(symbol=\"BTCUSDT\", interval=\"5m\", start_str=\"30 days ago UTC\"):\n",
    "    \"\"\"\n",
    "    python-binance's get_historical_klines automatically loops \n",
    "    for more than 500 bars, up to the current time.\n",
    "    \"\"\"\n",
    "    klines = client.get_historical_klines(\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        start_str=start_str,\n",
    "        limit=1500\n",
    "    )\n",
    "    if not klines:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "        \"close_time\", \"quote_volume\", \"trades\",\n",
    "        \"taker_base_vol\", \"taker_quote_vol\", \"ignore\"\n",
    "    ])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n",
    "        df[c] = df[c].astype(float)\n",
    "    df = df[[\"timestamp\",\"open\",\"high\",\"low\",\"close\",\"volume\"]].copy()\n",
    "    df.sort_values(\"timestamp\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "##############################################\n",
    "# 4. Data Pipeline with \"Math Olympiad\" Features\n",
    "##############################################\n",
    "\n",
    "def run_data_pipeline(symbol=\"BTCUSDT\", interval=\"5m\", days=30):\n",
    "    \"\"\"\n",
    "    Fetch data, compute multiple advanced features:\n",
    "     1) price_returns, fractional_diff\n",
    "     2) rolling Shannon entropy\n",
    "     3) skew, kurtosis, hurst\n",
    "     4) permutation entropy\n",
    "     5) wavelet energy\n",
    "     6) approximate Lyapunov exponent\n",
    "     7) Tsallis entropy\n",
    "    \"\"\"\n",
    "    print(\"=== Building Dataset ===\")\n",
    "    start_str = f\"{days} days ago UTC\"\n",
    "    ohlcv_df = fetch_ohlcv(symbol, interval, start_str=start_str)\n",
    "    print(\"OHLCV rows:\", len(ohlcv_df))\n",
    "\n",
    "    # Basic returns\n",
    "    ohlcv_df[\"price_returns\"] = ohlcv_df[\"close\"].pct_change()\n",
    "\n",
    "    # Fractional difference\n",
    "    ohlcv_df[\"frac_diff_returns\"] = fractional_diff(ohlcv_df[\"price_returns\"], d=0.4, max_lags=50)\n",
    "\n",
    "    # Rolling Shannon entropy\n",
    "    ohlcv_df[\"entropy\"] = rolling_shannon_entropy(ohlcv_df[\"price_returns\"], window=5, bins=5)\n",
    "\n",
    "    # Rolling Skew & Kurt\n",
    "    ohlcv_df[\"skew_30\"] = rolling_skewness(ohlcv_df[\"price_returns\"], window=30)\n",
    "    ohlcv_df[\"kurt_30\"] = rolling_kurtosis(ohlcv_df[\"price_returns\"], window=30)\n",
    "\n",
    "    # Rolling Hurst exponent\n",
    "    ohlcv_df[\"hurst_50\"] = rolling_hurst_exponent(ohlcv_df[\"price_returns\"], window=50)\n",
    "\n",
    "    # Permutation entropy\n",
    "    ohlcv_df[\"perm_entropy_10\"] = rolling_perm_entropy(ohlcv_df[\"price_returns\"], window=10, order=3)\n",
    "\n",
    "    # Wavelet energy (haar wavelet, level=2)\n",
    "    ohlcv_df[\"wavelet_energy\"] = rolling_wavelet_energy(ohlcv_df[\"price_returns\"], wavelet='haar', level=2, window=50)\n",
    "\n",
    "    # Largest Lyapunov exponent (approx)\n",
    "    ohlcv_df[\"lyapunov_50\"] = rolling_lyapunov_exponent(ohlcv_df[\"price_returns\"], window=50, tau=1, dim=2)\n",
    "\n",
    "    # Tsallis entropy\n",
    "    ohlcv_df[\"tsallis_30\"] = rolling_tsallis_entropy(ohlcv_df[\"price_returns\"], window=30, q=1.5)\n",
    "\n",
    "    print(\"Final rows (post-feature):\", len(ohlcv_df))\n",
    "    return ohlcv_df\n",
    "\n",
    "##############################################\n",
    "# 5. XGBoost Regressor (Focus on Model, No Backtest)\n",
    "##############################################\n",
    "\n",
    "def train_xgboost_regressor(df):\n",
    "    \"\"\"\n",
    "    Train an XGBoost regressor to predict next-candle return\n",
    "    from the advanced feature set. We won't do a backtest here,\n",
    "    just measure MSE / R^2 for the model.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Training XGBoost for Next-Candle Return Prediction ===\")\n",
    "\n",
    "    # Build a big feature list\n",
    "    features = [\n",
    "        \"frac_diff_returns\",\n",
    "        \"entropy\",\n",
    "        \"skew_30\",\n",
    "        \"kurt_30\",\n",
    "        \"hurst_50\",\n",
    "        \"perm_entropy_10\",\n",
    "        \"wavelet_energy\",\n",
    "        \"lyapunov_50\",\n",
    "        \"tsallis_30\",\n",
    "    ]\n",
    "\n",
    "    # Define target = next candle's return\n",
    "    df[\"next_return\"] = df[\"price_returns\"].shift(-1)\n",
    "\n",
    "    # Drop rows with NaNs in features or target\n",
    "    print(\"Before dropna, rows:\", len(df))\n",
    "    df.dropna(subset=features + [\"next_return\"], inplace=True)\n",
    "    print(\"After dropna, rows:\", len(df))\n",
    "\n",
    "    if len(df) < 10:\n",
    "        raise ValueError(\"Not enough data left after dropping NaNs!\")\n",
    "\n",
    "    X = df[features]\n",
    "    y = df[\"next_return\"]\n",
    "\n",
    "    # For a robust approach, use TimeSeriesSplit or walk-forward.\n",
    "    # But here we do a simple train_test_split for demonstration.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Test MSE: {mse:.8f}\")\n",
    "    print(f\"Test R^2: {r2:.6f}\")\n",
    "\n",
    "    # Feature importances\n",
    "    feat_imp = model.get_booster().get_score(importance_type=\"weight\")\n",
    "    print(\"Feature Importance:\", feat_imp)\n",
    "\n",
    "    return model\n",
    "\n",
    "#############################################################\n",
    "# 6. Example Notebook Usage (Focus on Model, Not Backtest)\n",
    "#############################################################\n",
    "\n",
    "symbol = \"BTCUSDT\"\n",
    "interval = \"5m\"\n",
    "days = 50  # or 30, etc. Increase for more data\n",
    "\n",
    "# 1) Build dataset with advanced \"math olympiad\" features\n",
    "df_ohlcv = run_data_pipeline(symbol, interval, days=days)\n",
    "\n",
    "# 2) Train XGBoost Regressor\n",
    "model = train_xgboost_regressor(df_ohlcv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
